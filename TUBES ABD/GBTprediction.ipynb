{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.util import MLUtils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col, concat, to_timestamp, year, month, day, hour, sum, concat_ws, lit, to_date, ceil, dayofweek, when, lag, isnull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Motor Vehicle Collisions\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CRASH DATE: string (nullable = true)\n",
      " |-- NUMBER OF PERSONS INJURED: string (nullable = true)\n",
      " |-- NUMBER OF PERSONS KILLED: integer (nullable = true)\n",
      " |-- NUMBER OF PEDESTRIANS INJURED: integer (nullable = true)\n",
      " |-- NUMBER OF PEDESTRIANS KILLED: integer (nullable = true)\n",
      " |-- NUMBER OF CYCLIST INJURED: integer (nullable = true)\n",
      " |-- NUMBER OF CYCLIST KILLED: string (nullable = true)\n",
      " |-- NUMBER OF MOTORIST INJURED: string (nullable = true)\n",
      " |-- NUMBER OF MOTORIST KILLED: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = spark.read.csv(\"Motor_Vehicle_Collisions_-_Crashes.csv\", header=True, inferSchema=True)\n",
    "data = data.select(\"CRASH DATE\",\"NUMBER OF PERSONS INJURED\", \"NUMBER OF PERSONS KILLED\", \"NUMBER OF PEDESTRIANS INJURED\", \"NUMBER OF PEDESTRIANS KILLED\", \"NUMBER OF CYCLIST INJURED\", \"NUMBER OF CYCLIST KILLED\", \"NUMBER OF MOTORIST INJURED\", \"NUMBER OF MOTORIST KILLED\")\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+----+-----+---+------------+-------------+------------+-------------+\n",
      "|CRASH DATE|NUMBER OF PERSONS INJURED|NUMBER OF PERSONS KILLED|NUMBER OF PEDESTRIANS INJURED|NUMBER OF PEDESTRIANS KILLED|NUMBER OF CYCLIST INJURED|NUMBER OF CYCLIST KILLED|NUMBER OF MOTORIST INJURED|NUMBER OF MOTORIST KILLED|YEAR|MONTH|DAY|TOTAL_KILLED|TOTAL_INJURED|TOTAL KILLED|TOTAL INJURED|\n",
      "+----------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+----+-----+---+------------+-------------+------------+-------------+\n",
      "|2021-09-11|                        2|                       0|                            0|                           0|                        0|                       0|                         2|                        0|2021|    9| 11|         0.0|          4.0|           0|            4|\n",
      "|2022-03-26|                        1|                       0|                            0|                           0|                        0|                       0|                         1|                        0|2022|    3| 26|         0.0|          2.0|           0|            2|\n",
      "|2022-06-29|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|2022|    6| 29|         0.0|          0.0|           0|            0|\n",
      "|2021-09-11|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|2021|    9| 11|         0.0|          0.0|           0|            0|\n",
      "|2021-12-14|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|2021|   12| 14|         0.0|          0.0|           0|            0|\n",
      "+----------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+----+-----+---+------------+-------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Typecast CRASH DATE and CRASH TIME into a single timestamp column\n",
    "data = data.withColumn(\"CRASH DATE\", to_date(col(\"CRASH DATE\"), \"MM/dd/yyyy\"))\n",
    "# Extract year, month, day, and hour from the timestamp column\n",
    "data = data.withColumn(\"YEAR\", year(col(\"CRASH DATE\")))\n",
    "data = data.withColumn(\"MONTH\", month(col(\"CRASH DATE\")))\n",
    "data = data.withColumn(\"DAY\", day(col(\"CRASH DATE\")))\n",
    "# Calculate the total number of killed and injured persons\n",
    "data = data.withColumn(\"TOTAL_KILLED\", col(\"NUMBER OF PERSONS KILLED\") + col(\"NUMBER OF PEDESTRIANS KILLED\") + col(\"NUMBER OF CYCLIST KILLED\") + col(\"NUMBER OF MOTORIST KILLED\"))\n",
    "data = data.withColumn(\"TOTAL_INJURED\", col(\"NUMBER OF PERSONS INJURED\") + col(\"NUMBER OF PEDESTRIANS INJURED\") + col(\"NUMBER OF CYCLIST INJURED\") + col(\"NUMBER OF MOTORIST INJURED\"))\n",
    "# Melakukan ceil pada output\n",
    "data = data.withColumn(\"TOTAL KILLED\", ceil(col(\"TOTAL_KILLED\")))\n",
    "data = data.withColumn(\"TOTAL INJURED\", ceil(col(\"TOTAL_INJURED\")))\n",
    "data = data.fillna(0, subset=[\"TOTAL KILLED\", \"TOTAL INJURED\"])\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+------------+-------------+\n",
      "|YEAR|MONTH|DAY|TOTAL KILLED|TOTAL INJURED|\n",
      "+----+-----+---+------------+-------------+\n",
      "|2021|    9| 11|           0|            4|\n",
      "|2022|    3| 26|           0|            2|\n",
      "|2022|    6| 29|           0|            0|\n",
      "|2021|    9| 11|           0|            0|\n",
      "|2021|   12| 14|           0|            0|\n",
      "|2021|    4| 14|           0|            0|\n",
      "|2021|   12| 14|           0|            0|\n",
      "|2021|   12| 14|           0|            4|\n",
      "|2021|   12| 14|           0|            0|\n",
      "|2021|   12| 14|           0|            0|\n",
      "|2021|   12| 13|           0|            0|\n",
      "|2021|   12| 14|           0|            0|\n",
      "|2021|   12| 14|           0|            0|\n",
      "|2021|   12| 14|           0|            0|\n",
      "|2021|   12| 14|           0|            4|\n",
      "|2021|   12| 14|           0|            0|\n",
      "|2021|   12| 14|           0|            8|\n",
      "|2021|   12| 14|           0|            6|\n",
      "|2021|   12| 11|           0|            2|\n",
      "|2021|   12| 14|           0|            0|\n",
      "+----+-----+---+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.select(\"YEAR\", \"MONTH\", \"DAY\", \"TOTAL KILLED\", \"TOTAL INJURED\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- TOTAL KILLED: long (nullable = true)\n",
      " |-- TOTAL INJURED: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.select(\"YEAR\", \"MONTH\", \"DAY\", \"TOTAL KILLED\", \"TOTAL INJURED\")\n",
    "data = data.dropna()\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=[\"YEAR\", \"MONTH\", \"DAY\"], outputCol=\"features\")\n",
    "data = vectorAssembler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "train_data = data.filter(data[\"YEAR\"] < 2020)\n",
    "test_data = data.filter(data[\"YEAR\"] > 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparameters\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"TOTAL KILLED\", maxIter=10, maxDepth=10, seed=42, lossType=\"squared\")\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxDepth, [5, 10]).addGrid(gbt.maxBins, [50, 100]).addGrid(gbt.minInstancesPerNode, [10, 50]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "crossval = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=RegressionEvaluator(labelCol=\"TOTAL KILLED\"), numFolds=3)\n",
    "cvModel = crossval.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.108018\n"
     ]
    }
   ],
   "source": [
    "#evaluate model\n",
    "predictions = cvModel.transform(test_data)\n",
    "evaluator = RegressionEvaluator(labelCol=\"TOTAL KILLED\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during prediction: An error occurred while calling o9290.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3842.0 failed 1 times, most recent failure: Lost task 2.0 in stage 3842.0 (TID 15249) (DESKTOP-V4CS9LO.mshome.net executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\n",
      "\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\n",
      "\tat java.net.PlainSocketImpl.accept(Unknown Source)\n",
      "\tat java.net.ServerSocket.implAccept(Unknown Source)\n",
      "\tat java.net.ServerSocket.accept(Unknown Source)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\n",
      "\t... 32 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4148)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4145)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Unknown Source)\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\t... 1 more\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\n",
      "\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\n",
      "\tat java.net.PlainSocketImpl.accept(Unknown Source)\n",
      "\tat java.net.ServerSocket.implAccept(Unknown Source)\n",
      "\tat java.net.ServerSocket.accept(Unknown Source)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\n",
      "\t... 32 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Forecasting\n",
    "futureData = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"YEAR\": [2025, 2025, 2025, 2025, 2025],\n",
    "            \"MONTH\": [1, 3, 5, 7, 9],\n",
    "            \"DAY\": [2, 4, 6, 8, 10]\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"YEAR\", \"MONTH\", \"DAY\",], outputCol=\"features\")\n",
    "futureData = vectorAssembler.transform(futureData)\n",
    "\n",
    "# Pastikan cvModel sudah dilatih sebelumnya dan siap untuk prediksi\n",
    "try:\n",
    "    predictions = cvModel.transform(futureData)\n",
    "    predictedKilled = predictions.select(\"prediction\").toPandas()\n",
    "    print(\"Predicted total killed for next 5 years:\", predictedKilled.values.tolist())\n",
    "except Exception as e:\n",
    "    print(\"Error during prediction:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HoldOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "# Define hyperparameters\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"TOTAL KILLED\", maxDepth=5, maxBins=50, minInstancesPerNode=1)\n",
    "\n",
    "# Train model\n",
    "model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+------------+-------------+------------------+--------------------+\n",
      "|YEAR|MONTH|DAY|TOTAL KILLED|TOTAL INJURED|          features|          prediction|\n",
      "+----+-----+---+------------+-------------+------------------+--------------------+\n",
      "|2012|    9| 25|           0|            0| [2012.0,9.0,25.0]|0.003468517870580...|\n",
      "|2016|    6|  6|           0|            0|  [2016.0,6.0,6.0]|0.002171788876053...|\n",
      "|2017|   10| 11|           0|            0|[2017.0,10.0,11.0]|0.002426131389444...|\n",
      "|2017|   11|  9|           0|            0| [2017.0,11.0,9.0]|0.004320031503587245|\n",
      "|2017|   11| 10|           0|            0|[2017.0,11.0,10.0]|0.002829747403615473|\n",
      "|2017|   11| 15|           0|            0|[2017.0,11.0,15.0]|0.002294930407449...|\n",
      "|2017|   11| 20|           0|            0|[2017.0,11.0,20.0]|0.002094381080981...|\n",
      "|2017|   11| 20|           0|            0|[2017.0,11.0,20.0]|0.002094381080981...|\n",
      "|2017|   11| 22|           0|            0|[2017.0,11.0,22.0]|0.002137663845437275|\n",
      "|2017|   11| 22|           0|            0|[2017.0,11.0,22.0]|0.002137663845437275|\n",
      "|2017|   11| 22|           0|            0|[2017.0,11.0,22.0]|0.002137663845437275|\n",
      "|2017|   11| 24|           0|            0|[2017.0,11.0,24.0]|0.002204534365488936|\n",
      "|2017|   11| 25|           0|            0|[2017.0,11.0,25.0]|0.001991289457625845|\n",
      "|2017|   11| 29|           0|            0|[2017.0,11.0,29.0]|0.001896657521783527|\n",
      "|2018|    3| 22|           0|            2| [2018.0,3.0,22.0]|0.001877904074413...|\n",
      "|2018|    5| 28|           0|            2| [2018.0,5.0,28.0]|0.002223731165002...|\n",
      "|2018|    8| 18|           0|            0| [2018.0,8.0,18.0]|0.002675486548961811|\n",
      "|2018|    8| 24|           0|            0| [2018.0,8.0,24.0]|0.002193684968556...|\n",
      "|2018|    8| 28|           0|            0| [2018.0,8.0,28.0]|0.002115979053368862|\n",
      "|2018|    9| 12|           0|            0| [2018.0,9.0,12.0]|0.002324175806978759|\n",
      "+----+-----+---+------------+-------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create prediction on test data\n",
    "prediction = model.transform(test_data)\n",
    "\n",
    "# Show the prediction results\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 0.08067391746596747\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"TOTAL KILLED\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(prediction)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
